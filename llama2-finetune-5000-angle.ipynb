{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea2ba3d-aeee-45f2-bbf7-4ea682628a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef96c641-35f7-4d81-b903-6fba4e38451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: kill: (4067862) - No such process\n",
      "Files removed: 12\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/huuthanhvy.nguyen001/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!kill -9 4067862\n",
    "\n",
    "!pip cache purge\n",
    "!rm -rf ~/.cache/huggingface\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/huuthanhvy.nguyen001/hpcstor6/LLMP/LLMP/')\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login ('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')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4124164-3568-4f5f-95f4-f524e672c0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                            Size  Used Avail Use% Mounted on\n",
      "udev                                                 1008G     0 1008G   0% /dev\n",
      "tmpfs                                                 202G   60M  202G   1% /run\n",
      "/dev/md0                                              1.8T  237G  1.4T  15% /\n",
      "tmpfs                                                1008G   25M 1008G   1% /dev/shm\n",
      "tmpfs                                                 5.0M     0  5.0M   0% /run/lock\n",
      "tmpfs                                                1008G     0 1008G   0% /sys/fs/cgroup\n",
      "/dev/md1                                               25T   23T  791G  97% /raid\n",
      "/dev/loop2                                             56M   56M     0 100% /snap/core18/2829\n",
      "/dev/nvme3n1p1                                        511M  6.1M  505M   2% /boot/efi\n",
      "/dev/loop4                                             92M   92M     0 100% /snap/lxd/29619\n",
      "/dev/loop3                                             64M   64M     0 100% /snap/core20/2318\n",
      "/dev/loop5                                             92M   92M     0 100% /snap/lxd/24061\n",
      "/dev/loop7                                             39M   39M     0 100% /snap/snapd/21759\n",
      "10.0.5.249:/opt/ohpc/pub                              200G  112G   89G  56% /opt/ohpc/pub\n",
      "10.1.4.245:/hpcstor6/scratch01                        1.9T  621G  1.3T  33% /hpcstor6/scratch01\n",
      "10.0.5.249:/share                                     500G  225G  276G  45% /share\n",
      "10.1.4.249:/hpcstor1/scratch01                         11T  6.4T  4.0T  62% /hpcstor1/scratch01\n",
      "10.1.4.245:/hpcstor6/share_home                       3.6T  6.8G  3.6T   1% /home\n",
      "10.0.5.249:/opt/ohpc/admin/spack                      200G  112G   89G  56% /opt/ohpc/admin/spack\n",
      "10.1.4.246:/hpcstor4/data01                            28T  9.1G   26T   1% /hpcstor4/data01\n",
      "10.0.5.249:/opt/ohpc/admin/modulefiles                200G  112G   89G  56% /opt/ohpc/admin/modulefiles\n",
      "10.1.4.242:/mathspace/data01                           57T   45T   12T  80% /mathspace/data01\n",
      "/dev/loop8                                             64M   64M     0 100% /snap/core20/2379\n",
      "/dev/loop1                                             56M   56M     0 100% /snap/core18/2846\n",
      "/dev/loop0                                             45M   45M     0 100% /snap/snapd/22991\n",
      "10.1.4.245:/hpcstor6/share_home/huuthanhvy.nguyen001  100G   74G   27G  74% /home/huuthanhvy.nguyen001\n",
      "10.1.4.245:/hpcstor6/share_home/youxiang.zhu001       100G   78G   23G  78% /home/youxiang.zhu001\n",
      "10.1.4.245:/hpcstor6/share_home/jacob.adamczyk001     100G   64G   37G  64% /home/jacob.adamczyk001\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ~/.cache/huggingface/transformers/*\n",
    "!df -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14a78a8-aebb-4e8b-b7f0-ed02dec160ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded successfully!\n",
      "{'id': 'ddf7eaf9-75b5-4512-8362-dab051c98a61', 'image': 'ddf7eaf9-75b5-4512-8362-dab051c98a61.jpg', 'angle': 'The line drawing in the image forms an acute angle. The exact angle in this picture is 129 degrees.'}\n",
      "Validation data loaded successfully!\n",
      "{'id': 'ecfba847-26f1-4dda-82e1-1983b913e123', 'image': 'ecfba847-26f1-4dda-82e1-1983b913e123.jpg', 'angle': 'The line drawing in the image forms an acute angle. The exact angle in this picture is 255 degrees.'}\n",
      "==== Transform to Dataset Format ====\n",
      "Transformed Training Dataset:\n",
      "Dataset({\n",
      "    features: ['image', 'angle'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Transformed Validation Dataset:\n",
      "Dataset({\n",
      "    features: ['image', 'angle'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Define the paths to the train and validation datasets\n",
    "train_dataset_path = './finetuningDataset5000angle/train/dataset.json'\n",
    "validation_dataset_path = './finetuningDataset5000angle/validation/dataset.json'\n",
    "\n",
    "# ======= Load the training dataset =======\n",
    "with open(train_dataset_path, 'r') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Now 'train_data' holds your training dataset\n",
    "print(\"Training data loaded successfully!\")\n",
    "print(train_data[0])  # Print the first entry to verify\n",
    "\n",
    "# ======= Load the validation dataset =======\n",
    "with open(validation_dataset_path, 'r') as file:\n",
    "    validation_data = json.load(file)\n",
    "\n",
    "# Now 'validation_data' holds your validation dataset\n",
    "print(\"Validation data loaded successfully!\")\n",
    "print(validation_data[0])  # Print the first entry to verify\n",
    "\n",
    "# ======= Transform to Dataset Format =======\n",
    "print('==== Transform to Dataset Format ====')\n",
    "\n",
    "# Transform the loaded JSON data into the Dataset format used by the 'datasets' library\n",
    "# For both train and validation datasets\n",
    "\n",
    "# Process training dataset\n",
    "train_dataset = [\n",
    "    {\n",
    "        'image': item['image'],   # Path to the image\n",
    "        'angle': item['angle']    # The angle value (label)\n",
    "    }\n",
    "    for item in train_data\n",
    "]\n",
    "\n",
    "# Process validation dataset\n",
    "validation_dataset = [\n",
    "    {\n",
    "        'image': item['image'],   # Path to the image\n",
    "        'angle': item['angle']    # The angle value (label)\n",
    "    }\n",
    "    for item in validation_data\n",
    "]\n",
    "\n",
    "# Create Hugging Face datasets from the list of dictionaries\n",
    "train_data = Dataset.from_list(train_dataset)\n",
    "validation_data = Dataset.from_list(validation_dataset)\n",
    "\n",
    "# Display the transformed datasets\n",
    "print(\"Transformed Training Dataset:\")\n",
    "print(train_data)\n",
    "\n",
    "print(\"Transformed Validation Dataset:\")\n",
    "print(validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffc274-2c0a-44af-b300-352d731ee07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import evaluate\n",
    "\n",
    "def process(examples):\n",
    "    texts = [\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>{item['image']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{item['angle']}<|eot_id|>\"\n",
    "        for item in examples\n",
    "    ]\n",
    "\n",
    "    image_folder = \"/home/huuthanhvy.nguyen001/hpcstor6/LLMP/LLMP/LLMP/finetuningDataset50angle/images\"\n",
    "    # Resize the images to 100x100 using Lanczos filter and convert to RGB\n",
    "    images = [\n",
    "        Image.open(os.path.join(image_folder, item[\"image\"]))\n",
    "             .resize((100, 100), Image.LANCZOS)  # Resize the image to 100x100\n",
    "             .convert(\"RGB\")  # Convert the resized image to RGB\n",
    "        for item in examples\n",
    "    ]\n",
    "\n",
    "    # Assuming `processor` is defined elsewhere in the code\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Reshape pixel_values to [batch_size, width, height, color_channels]\n",
    "    #pixel_values = batch['pixel_values']  # Original shape: [1, 1, 4, 3, 560, 560]\n",
    "    #pixel_values = pixel_values.squeeze(0)  # Shape: [1, 4, 3, 560, 560]\n",
    "    #pixel_values = pixel_values.permute(0, 1, 3, 4, 2)  # Shape: [1, 4, 560, 560, 3]\n",
    "    #batch['pixel_values'] = pixel_values  # Update the batch with reshaped pixel_values\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == 128256] = -100  # image token index\n",
    "    batch[\"labels\"] = labels\n",
    "    batch = batch.to(torch.bfloat16).to(\"cuda\")\n",
    "    \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e337fcf2-ca51-4d88-8f76-64d141be48a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfirst_example = process([train_data[0]])  # Process the first example\\nprint(\"Input IDs shape:\", first_example[\\'input_ids\\'].shape)\\nprint(\"Attention mask shape:\", first_example[\\'attention_mask\\'].shape)\\nprint(\"Pixel values shape:\", first_example[\\'pixel_values\\'].shape)\\nprint(\"Labels shape:\", first_example[\\'labels\\'].shape)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "first_example = process([train_data[0]])  # Process the first example\n",
    "print(\"Input IDs shape:\", first_example['input_ids'].shape)\n",
    "print(\"Attention mask shape:\", first_example['attention_mask'].shape)\n",
    "print(\"Pixel values shape:\", first_example['pixel_values'].shape)\n",
    "print(\"Labels shape:\", first_example['labels'].shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96277ec6-1511-408d-bdd2-132db9d2995d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['cache_dir']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906c9bff5acb45198d20bac59a5d5158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb929f884f6a4b75a51aad4e72cf046f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddd216b8f984b149e632206ea7ef170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8b0e55b17c4b66a7a59d9006bcf214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/5.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33227b7b7d24469e98a0e4aefed3658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa3e9e797ea471eb790985f5580c092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120dc8faf79046f6b3a7b5755fe51418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/5.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Function to set requires_grad for trainable parameters\\ndef set_trainable_params(model, target_modules):\\n    for name, param in model.named_parameters():\\n        if any(module in name for module in target_modules):\\n            # Ensure the parameter is of a trainable data type\\n            if param.dtype in [torch.float32, torch.float64, torch.bfloat16]:\\n                param.requires_grad = True\\n            else:\\n                # Convert to a trainable data type if necessary\\n                param.data = param.data.float()\\n                param.requires_grad = True\\n                print(f\"Set requires_grad=True for {name}\")\\n        else:\\n            param.requires_grad = False\\n            print(f\"Set requires_grad=False for {name}\")\\n\\n\\n# Set requires_grad for the specified modules\\nset_trainable_params(model, peft_config.target_modules)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "# Load Bits and Bytes Configuration for quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    cache_dir=\"/tmp\"\n",
    ")\n",
    "\n",
    "# Load the model with quantization\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    cache_dir=\"/tmp\"\n",
    ")\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.5,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Layers to fine-tune\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the loaded model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\"\"\"\n",
    "# Function to set requires_grad for trainable parameters\n",
    "def set_trainable_params(model, target_modules):\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(module in name for module in target_modules):\n",
    "            # Ensure the parameter is of a trainable data type\n",
    "            if param.dtype in [torch.float32, torch.float64, torch.bfloat16]:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                # Convert to a trainable data type if necessary\n",
    "                param.data = param.data.float()\n",
    "                param.requires_grad = True\n",
    "                print(f\"Set requires_grad=True for {name}\")\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "            print(f\"Set requires_grad=False for {name}\")\n",
    "\n",
    "\n",
    "# Set requires_grad for the specified modules\n",
    "set_trainable_params(model, peft_config.target_modules)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2aae844-8040-4d40-86e9-006de2b8d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,796,480 || all params: 10,682,017,315 || trainable%: 0.1104\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbd04d4-7fbf-4348-be1f-c57757285742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huuthanhvy.nguyen001/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/huuthanhvy.nguyen001/anaconda3/envs/pytorch/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 8:03:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.634100</td>\n",
       "      <td>1.673096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.632000</td>\n",
       "      <td>1.625402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.625500</td>\n",
       "      <td>1.624560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.627200</td>\n",
       "      <td>1.623286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.623300</td>\n",
       "      <td>1.623561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.622000</td>\n",
       "      <td>1.623646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.619400</td>\n",
       "      <td>1.633191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.613000</td>\n",
       "      <td>1.644686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.619100</td>\n",
       "      <td>1.637587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.623000</td>\n",
       "      <td>1.642379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.615100</td>\n",
       "      <td>1.648931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.616500</td>\n",
       "      <td>1.654103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.613200</td>\n",
       "      <td>1.648775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.612000</td>\n",
       "      <td>1.646506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.616100</td>\n",
       "      <td>1.649016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.616900</td>\n",
       "      <td>1.654158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.611100</td>\n",
       "      <td>1.653560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.612100</td>\n",
       "      <td>1.653376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.6730961799621582 at step 100\n",
      "Validation loss: 1.6254016160964966 at step 200\n",
      "Validation loss: 1.6245602369308472 at step 300\n",
      "Validation loss: 1.623286485671997 at step 400\n",
      "Validation loss: 1.6235610246658325 at step 500\n",
      "Validation loss: 1.6236460208892822 at step 600\n",
      "Validation loss: 1.6331911087036133 at step 700\n",
      "Validation loss: 1.6446858644485474 at step 800\n",
      "Validation loss: 1.637587308883667 at step 900\n",
      "Validation loss: 1.6423791646957397 at step 1000\n",
      "Validation loss: 1.648930549621582 at step 1100\n",
      "Validation loss: 1.6541026830673218 at step 1200\n",
      "Validation loss: 1.648775339126587 at step 1300\n",
      "Validation loss: 1.6465060710906982 at step 1400\n",
      "Validation loss: 1.6490161418914795 at step 1500\n",
      "Validation loss: 1.6541577577590942 at step 1600\n",
      "Validation loss: 1.6535595655441284 at step 1700\n",
      "Validation loss: 1.6533759832382202 at step 1800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e626ddcd60d4f06bc9f2a11b2e9c964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/47.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97e5261c4a34213b9cab2b31e23e63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38507622a2b44c184ab65e4ccbf72e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/raminguyen/llama-3.2-vision-instruct-5000-angle/commit/d90cd96e92cf909e383d3240dfdfd3e852d8e6cd', commit_message='Upload processor', commit_description='', oid='d90cd96e92cf909e383d3240dfdfd3e852d8e6cd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/raminguyen/llama-3.2-vision-instruct-5000-angle', endpoint='https://huggingface.co', repo_type='model', repo_id='raminguyen/llama-3.2-vision-instruct-5000-angle'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load the validation dataset the same way you load the train dataset\n",
    "# Assuming train_data and validation_data are already loaded datasets\n",
    "\n",
    "def process(examples):\n",
    "    texts = [\n",
    "        f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>{item['image']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{item['angle']}<|eot_id|>\"\n",
    "        for item in examples\n",
    "    ]\n",
    "\n",
    "    image_folder = \"/home/huuthanhvy.nguyen001/hpcstor6/LLMP/LLMP/LLMP/finetuningDataset5000angle/images\"\n",
    "    # Resize the images to 448x448 using Lanczos filter and convert to RGB\n",
    "    images = [\n",
    "        Image.open(os.path.join(image_folder, item[\"image\"]))\n",
    "             .resize((448, 448), Image.LANCZOS)  # Resize to 448x448 using Lanczos filter\n",
    "             .convert(\"RGB\")  # Convert the resized image to RGB\n",
    "        for item in examples\n",
    "    ]\n",
    "\n",
    "    # Assuming `processor` is defined elsewhere in the code\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == 128256] = -100  # image token index\n",
    "    batch[\"labels\"] = labels\n",
    "    batch = batch.to(torch.bfloat16).to(\"cuda\")\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Modify TrainingArguments to include evaluation strategy\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama-3.2-vision-instruct-5000-angle\",  # Updated name here\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",  # Evaluate at each logging step\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,  # Add batch size for evaluation\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta2=0.999,\n",
    "    max_grad_norm=1.0,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_hf\",\n",
    "    save_total_limit=1,\n",
    "    bf16=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# Custom callback to log both training and validation metrics\n",
    "class LogMetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_logs = []\n",
    "        self.validation_logs = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:  # Log training loss\n",
    "                self.training_logs.append((state.global_step, logs[\"loss\"]))\n",
    "            if \"eval_loss\" in logs:  # Log validation loss\n",
    "                self.validation_logs.append((state.global_step, logs[\"eval_loss\"]))\n",
    "                print(f\"Validation loss: {logs['eval_loss']} at step {state.global_step}\")\n",
    "\n",
    "# Initialize the custom callback\n",
    "log_metrics_callback = LogMetricsCallback()\n",
    "\n",
    "# Assuming `model` is loaded before this section\n",
    "model.tie_weights()  # Tie the weights after loading the model\n",
    "\n",
    "# Trainer setup including validation data\n",
    "trainer = Trainer(\n",
    "    model=model,  # Ensure 'model' is defined and initialized\n",
    "    args=training_args,\n",
    "    data_collator=process,  # Assuming 'process' is defined and appropriate\n",
    "    train_dataset=train_data,  # Assuming 'train_data' is defined\n",
    "    eval_dataset=validation_data,  # Add validation dataset\n",
    "    callbacks=[log_metrics_callback],\n",
    ")\n",
    "\n",
    "# Train the model with validation\n",
    "trainer.train()\n",
    "\n",
    "# === Step to Save the Entire Model ===\n",
    "\n",
    "# Save the merged model to the specified directory\n",
    "model.save_pretrained(\"llama-3.2-vision-instruct-5000-angle\")  # Updated name here\n",
    "\n",
    "# Push to Hugging Face Hub (optional)\n",
    "model.push_to_hub(\"llama-3.2-vision-instruct-5000-angle\")  # Updated name here\n",
    "\n",
    "# Push the processor to Hugging Face Hub (optional)\n",
    "processor.push_to_hub(\"llama-3.2-vision-instruct-5000-angle\")  # Ensure 'processor' is defined and initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27928f3b-70f4-4f48-b4a5-95a5c0bce1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Response: user\n",
      "\n",
      "the two line form creates angles. can you estimate angle degree in specific numberassistant\n",
      "\n",
      "The line drawing in the image forms an angle of approximately 47 degrees.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "input_image_path = \"/home/huuthanhvy.nguyen001/hpcstor6/LLMP/LLMP/LLMP/cb88d7bc-84ac-4b7f-b8d1-7f8ff3ab4b39.jpg\"\n",
    "\n",
    "image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Resize the image to the required dimensions (e.g., 448x448)\n",
    "image = image.resize((448, 448))\n",
    "\n",
    "input_question = \"the two line form creates angles. can you estimate angle degree in specific number\"\n",
    "\n",
    "# Properly format the input text\n",
    "text = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|image|>{input_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "\n",
    "# Prepare the input using the processor\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs = {key: value.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) for key, value in inputs.items()}\n",
    "\n",
    "# Proceed with inference using your model\n",
    "\n",
    "output = model.generate(**inputs, max_length=500)\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = processor.decode(output[0], max_new_tokens=50, skip_special_tokens=True)\n",
    "\n",
    "# Display the output\n",
    "print(\"Model's Response:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d28ed-53b2-412f-b748-7984015170fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Plot training and validation loss\n",
    "plt.plot([x[0] for x in log_metrics_callback.training_logs], [x[1] for x in log_metrics_callback.training_logs], label=\"Training Loss\")\n",
    "plt.plot([x[0] for x in log_metrics_callback.validation_logs], [x[1] for x in log_metrics_callback.validation_logs], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86f379-ccea-4498-9bad-cc8e038d86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda clean --all\n",
    "!rm -rf /home/huuthanhvy.nguyen001/wandb\n",
    "!rm /home/huuthanhvy.nguyen001/adapter_model.safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e5d95-8545-4a64-91cd-0eadc40a4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from huggingface_hub import login\n",
    "login ('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')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942275d-c00c-4126-adfc-8d41e8e31378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import matplotlib.pyplot as plt\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "# Custom callback to log training loss\n",
    "class LogMetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_logs = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.training_logs.append((state.epoch, logs[\"loss\"]))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Extract epochs and losses\n",
    "epochs, losses = zip(*log_metrics_callback.training_logs)\n",
    "\n",
    "# Plot the loss vs epochs\n",
    "plt.plot(epochs, losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs Epochs')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ba154-7792-4eb7-87f9-f93eb813492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import matplotlib.pyplot as plt\n",
    "\n",
    "# Training epochs and corresponding loss values\n",
    "epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Each epoch represents 500 steps\n",
    "loss = [1.434400, 0.165100, 0.121400, 0.111300, 0.105600, 0.100800, 0.097700, 0.093800, 0.091100, 0.088700]\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(epochs, loss, marker='o')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs (Each = 500 steps)')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.xticks(epochs) \n",
    "\n",
    "# Show the plot\n",
    "plt.show()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489baad6-d8a3-4339-9f52-b7e3c5e0858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"!nvidia-smi\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e3e78-4998-41fb-af85-df30a5ba786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"!git clone https://github.com/2U1/Llama3.2-Vision-Finetune.git\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837d6d4-b288-4e4f-b069-f025f005ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"!bash finetune_lora_vision.sh\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1498d39-2d7b-4c81-ae6f-35df1e4d7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "# Scan the Hugging Face cache directory\n",
    "cache_info = scan_cache_dir()\n",
    "\n",
    "# Delete the specific revision of the Llama-3.2-11B-Vision model\n",
    "delete_strategy = cache_info.delete_revisions(\n",
    "    \"3f2e93603aaa5dd142f27d34b06dfa2b6e97b8be\"\n",
    ")\n",
    "\n",
    "# See how much space will be freed\n",
    "print(\"Will free \" + delete_strategy.expected_freed_size_str)\n",
    "\n",
    "# Execute the deletion to free up the space\n",
    "delete_strategy.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebd404-a56d-4c48-911d-77996f74192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh ~/.cache/huggingface/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c48568-6356-498c-8f7e-0cb881cd0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CachedFileInfo()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a4afc9-b7d9-444a-b7bd-964233851043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8bf508721e4f4eb040f5a1b009cbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde6fa2677fd4487b1310cad7b675121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\n\\nmessages = [\\n    {\"role\": \"user\", \"content\": [\\n        {\"type\": \"image\"},\\n        {\"type\": \"text\", \"text\": \"If I had to write a haiku for this one, it would be: \"}\\n    ]}\\n]\\ninput_text = processor.apply_chat_template(messages, add_generation_prompt=True)\\ninputs = processor(\\n    image,\\n    input_text,\\n    add_special_tokens=False,\\n    return_tensors=\"pt\"\\n).to(model.device)\\n\\noutput = model.generate(**inputs, max_new_tokens=30)\\nprint(processor.decode(output[0]))\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"If I had to write a haiku for this one, it would be: \"}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=30)\n",
    "print(processor.decode(output[0]))\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849fb032-aa4f-4078-b28e-dae5b98a1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 28])\n",
      "Pixel values shape: torch.Size([1, 1, 4, 3, 560, 560])\n"
     ]
    }
   ],
   "source": [
    "# Check the input shapes\n",
    "print(\"Input IDs shape:\", inputs['input_ids'].shape)\n",
    "print(\"Pixel values shape:\", inputs['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095120d9-d47b-49d5-a344-d4d39fccf5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check the input shapes\n",
    "print(\"Input IDs shape:\", inputs['input_ids'].shape)\n",
    "print(\"Pixel values shape:\", inputs['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f2f57-0f01-42a0-a486-9dd025f31688",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/huuthanhvy.nguyen001/anaconda3/envs/pytorch/lib/python3.11/site-packages/huggingface_hub\n",
    "!rm -rf ~/.cache/huggingface/*\n",
    "!rm -rf ~/.cache/transformers/*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
